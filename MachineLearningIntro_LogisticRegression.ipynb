{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sb\n",
    "import time\n",
    "\n",
    "#\n",
    "# To run this code:\n",
    "# Make sure you have the ml virtualenvironment built and you are working on it.\n",
    "# Start the jupyter notebook within the virtualenv:\n",
    "# > workon ml; ipython notebook\n",
    "#\n",
    "\n",
    "# Dark style to be visualized easier\n",
    "sb.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook walks through an example of building a logistic regression classifier. We walk through the specification of the model, how we get the loss function, and provide two methods for optimizing (one we build ourselves, and another using a standard package).\n",
    "\n",
    "### Logistic Regression Background\n",
    "Logistic regression is defined as follows:\n",
    "\n",
    "$$p(x) = \\frac{1}{1 + exp(\\sum_j \\beta_j x_j)}$$\n",
    "\n",
    "$x$ is the input feature vector we'd like to classify.\n",
    "\n",
    "$\\beta$ is a vector of weights, these are the model parameters we will learn from our training data.\n",
    "\n",
    "$p(x)$ is the resulting probability of $x$ belonging to the positive class.\n",
    "\n",
    "**Note**: While it is possible to extend logistic regression to perform multi-class classification, this tutorial focused only on the 2 class problem for clarity.\n",
    "\n",
    "For a more in-depth reference to logistic regression check out this [reference](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf).\n",
    "\n",
    "**Quiz** Why is it called \"logistic regression\" if we use it to do classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# plot the logistic function\n",
    "#\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "# TODO: tweak the parameters and see how the function changes, \n",
    "# Can you decrease/increase the slope?\n",
    "# Can you change the normalization from [0,1] to [2, 10]?\n",
    "# Can you shift the point at which y = 0.5 from x = 0 to x = 5?\n",
    "y = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(17,7))\n",
    "ax.set_title('Logistic function', fontsize=16)\n",
    "\n",
    "plt.plot(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Helper methods\n",
    "#\n",
    "def logistic(betas, feature_vectors):\n",
    "    x = np.array([np.sum(betas * fv) for fv in feature_vectors])\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def classify(betas, feature_vectors):\n",
    "    out = logistic(betas, feature_vectors)\n",
    "    out_class = out >= 0.5\n",
    "    return out_class\n",
    "\n",
    "\n",
    "def gen_class(n, mu, sig):\n",
    "    return np.random.multivariate_normal(mu, np.eye(len(mu)) * sig, n)\n",
    "\n",
    "\n",
    "# takes an input feature vector and adds a 1 to the end of it\n",
    "def add_bias(data):\n",
    "    data1 = np.array(data[:])\n",
    "    N = len(data1)\n",
    "\n",
    "    if len(data1.shape) == 1:\n",
    "        data1 = data1.reshape(1, N)\n",
    "        data1 = np.append(data1, np.ones((1, 1)), axis=1)\n",
    "    else:\n",
    "        data1 = np.append(data1, np.ones((N, 1)), axis=1)\n",
    "    return data1\n",
    "\n",
    "\n",
    "def accuracy(data, data_class, betas):\n",
    "    pred_class = classify(betas, data)\n",
    "    return np.sum(pred_class == data_class) / (1.0 * len(data_class))\n",
    "\n",
    "\n",
    "def test_result(beta_learned, n):\n",
    "    print('\\ntest data points from class x1')\n",
    "    for i in range(n):\n",
    "        x_test1 = add_bias(gen_class(1, MU1, SIG1))\n",
    "        print(classify(beta_learned, x_test1)[0], logistic(beta_learned, x_test1)[0])\n",
    "\n",
    "    print('\\ntest data points from class x2')\n",
    "    for i in range(n):\n",
    "        x_test2 = add_bias(gen_class(1, MU2, SIG2))\n",
    "        print(classify(beta_learned, x_test2)[0], logistic(beta_learned, x_test2)[0])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Generate some data\n",
    "#\n",
    "\n",
    "# number of points in each class\n",
    "N = 100\n",
    "\n",
    "# define parameters for test data\n",
    "# we're generating from a 2d gaussian distribution, so this is the mean vector and the variance\n",
    "MU1 = [3, 0]\n",
    "SIG1 = 1\n",
    "\n",
    "MU2 = [0, 3]\n",
    "SIG2 = 1\n",
    "# TODO: play around with these parameters to see how the classification changes. \n",
    "# what happens when the classes are not linearly separable?\n",
    "\n",
    "# generate 2-d gaussian data with these parameters\n",
    "X1 = gen_class(N, MU1, SIG1)\n",
    "X2 = gen_class(N, MU2, SIG2)\n",
    "data = np.concatenate((X1, X2))\n",
    "\n",
    "# define the classes\n",
    "Y1 = np.ones((N, 1)) * 0\n",
    "Y2 = np.ones((N, 1)) * 1\n",
    "data_class = np.concatenate((Y1, Y2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Do a bunch of stuff here to plot the data\n",
    "#\n",
    "import matplotlib.cm as cm\n",
    "colors = cm.rainbow(np.linspace(0, 1, 2))\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "ax.set_title('Dataset', fontsize=16)\n",
    "# plot class 1, then class 2\n",
    "plt.scatter(X1[:, 0], X1[:, 1], s=50, c=colors[0], label='class 1')\n",
    "plt.scatter(X2[:, 0], X2[:, 1], s=50, c=colors[1], label='class 2')\n",
    "legend = ax.legend(loc='upper left', fontsize=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build our learning procedure. That is, given our logistic regression model, how can we identify the most appropriate set of weight parameters such that we are able to correctly classify our dataset. To do this, we'll need a bit of math.\n",
    "\n",
    "Remember, the logistic regressor is defined as:\n",
    "$$p(x_i) = \\frac{1}{1 + exp(\\sum_j \\beta_j x_{i,j})}$$\n",
    "\n",
    "This is just plugging a record, $x_i$ into the logistic regression model, where we use $\\beta$ to weight each feature in $x_i$ in order to get our class prediction (i.e. probability of being a \"positive\" example).\n",
    "\n",
    "To learn the appropriate parameters, we will need to define an objective function (sometimes called a loss function) over which we wish to optimize. The objective defines how we go about scoring a model, so we can quantify how good or bad each set of learned parameters is. Once we have this function, we will discuss how to go about optimizing with respect to $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to classify data into one of two classes, so we want to identify an objective function that correctly scores models that classify correctly better than models that make mistakes. There are a couple of ways to go about specifying an objective function and learning the weight parameters, but we will walk through one approach using the [binomial likelihood function](https://en.wikipedia.org/wiki/Binomial_distribution).\n",
    "\n",
    "#### Likelihood function\n",
    "We will define our likelihood function as follows:\n",
    "\n",
    "$$like = \\prod_{i} p_i^{y_i} (1 - p_i)^{(1 - y_i)}$$\n",
    "\n",
    "To learn the appropriate parameters for our model, we can take the derivative of our likelihood function with respect to each $\\beta_j$. To simplify the derivative, we first take the logarithm of this function (since maximizing the log-likelihood will also maximize the likelihood and it makes the math much simpler).\n",
    "\n",
    "$$llike = \\sum_{i} y_i log(p_i) + (1 - y_i) log(1 - p_i)$$\n",
    "\n",
    "Next, we'll take the derivative with respect to each component of $\\beta$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_j} \\sum_{i} y_i log(p_i) + (1 - y_i) log(1 - p_i) = 0$$\n",
    "\n",
    "After some algebraic manipulation, we find that:\n",
    "\n",
    "$$\\sum_{i} (y_i - p_i) x_{ij} = 0$$\n",
    "\n",
    "If we expand $p_i$, we can see that it is not possible so solve for $\\beta_j$ in a closed form, so we will use an iterative optimization algorithm that approximates the solution by continually moving each $\\beta_j$ in the direction of the steepest gradient at each step.\n",
    "\n",
    "\n",
    "#### Learning algorithm\n",
    "\n",
    "We can iteratively solve for (approximately) optimal parameters by nudging each $\\beta_j$ in the right direction:\n",
    "\n",
    "$$\\beta_j += \\alpha \\sum_{i} (y_i - p_i) x_{ij}$$\n",
    "\n",
    "We define a learning rate, $\\alpha$, to make sure that we are taking small enough steps (so we don't miss the minimum/maximum). Since we do not know the full form of the function, this ensures that we stay on the right path toward the optimum. If you notice the algorithm switching back and forth between parameter values, it usually means your step size is too large. \n",
    "\n",
    "The function defined below puts all of this together.\n",
    "\n",
    "If you are curious about the full derivation of the maximum likelihood approach, check out this [reference](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/lr_derivation1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/lr_derivation2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(data, data_class, use_ml=True, learn_iters=4000, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Method for learning logistic regression (LR) parameters\n",
    "\n",
    "    :param data: Feature vectors over which we'd like to learn, each row is a record, each column is a feature\n",
    "    :param data_class: Label for the associated record in data\n",
    "    :param learn_iters: Number of iterations to run through our optimization algorithm\n",
    "    :param alpha: Gradient step size (i.e. learning rate)\n",
    "    :return: Vector of learned weighting parameters for LR classifier\n",
    "    \"\"\"\n",
    "    # add bias to input feature vectors\n",
    "    data = add_bias(data)\n",
    "    num_rows, num_features = data.shape\n",
    "    data_class = np.reshape(data_class, (len(data_class),))\n",
    "\n",
    "    # initialize beta vector (LR Weights) to all zeros\n",
    "    betas = np.zeros(num_features)  # np.random.random(num_features)\n",
    "\n",
    "    #\n",
    "    # Iterate through the optimization\n",
    "    #\n",
    "    for iter in range(learn_iters):\n",
    "        # Given our current beta vector, attempt to classify the data\n",
    "        # this is our current guess, we'll then compare that to the true labels\n",
    "        # and attempt to update the beta vector so that we'll be better on the next iteration\n",
    "        guess = logistic(betas, data)\n",
    "\n",
    "        for i in range(len(betas)):\n",
    "            # compute the likelihood of the model\n",
    "            # we don't actually use this, just print it out for logging\n",
    "            llike = np.sum(data_class * np.log(guess) + (1 - data_class) * np.log(guess))\n",
    "\n",
    "            if use_ml:\n",
    "                # if we're using the loglikelihood as our objective function,\n",
    "                # this is the derivative for beta_i\n",
    "                diff = np.sum((data_class - guess) * data[:, i])\n",
    "                betas[i] += alpha * diff\n",
    "            else:\n",
    "                # if we're using the squared error as our objective function,\n",
    "                # this is the derivative of beta_i\n",
    "                betas[i] += alpha * 2 * np.sum((data_class - guess) * guess * (1 - guess) * data[:, i])\n",
    "                # note: the constant multiplication by 2 here does not matter. It is technically part of\n",
    "                # the derivative, but because its constant across any data and parameters, we could leave it\n",
    "                # out without any affect on the efficacy of the learner.\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            print(iter, betas, llike)\n",
    "\n",
    "    return betas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Let's do some learning!!\n",
    "#\n",
    "\n",
    "# It's finally time to put all of this together and turn it into something useful.\n",
    "# we'll give our learning algorithm our feature vectors and their corresponding labels\n",
    "# and get back the learned model parameters.\n",
    "s = time.time()\n",
    "betas = learn(data, data_class, alpha=0.05)\n",
    "e = time.time()\n",
    "\n",
    "# TODO: play around with the parameters for alpha to see how changing the learning rate influences the convergence\n",
    "# what if we had higher dimensional data?\n",
    "\n",
    "\n",
    "print('\\nDone learning... {}s'.format(e - s))\n",
    "print(betas) \n",
    "print('\\n==============\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# reshape data\n",
    "#\n",
    "data1 = add_bias(data)\n",
    "num_rows, num_features = data.shape\n",
    "data_class1 = np.reshape(data_class, (N * 2,))\n",
    "\n",
    "print('Accuracy on training data: {}'.format(accuracy(data1, data_class1, betas)))\n",
    "test_result(betas, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a logistic regression classifier! But what if we don't want to compute our own derivatives? In this case we can simply define our objective function (actually, in this case a loss function which we'll aim to minimize) and use a [standard package](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.minimize.html) to do the optimization. \n",
    "\n",
    "Of course there are easier ways to accomplish this, the simplest being [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), but the goal for this tutorial is to understand how things work!\n",
    "\n",
    "To implement our objective function we'll take the log likelihood we defined above and negate it since we're minimizing instead of maximizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(betas, data, data_class):\n",
    "    # avoid log(0) = -Inf\n",
    "    EPS = 1e-16\n",
    "    predict = logistic(betas, data)\n",
    "\n",
    "    # TODO: would a simpler objective function work instead of this one?\n",
    "    # log likelihood function\n",
    "    llike = -np.sum(data_class * np.log(predict + EPS) + (1 - data_class) * np.log(1 - predict + EPS))\n",
    "    return llike\n",
    "\n",
    "# TODO: how would the objective function change if you wanted to regularize the parameters?\n",
    "# https://www.quora.com/What-is-regularization-in-machine-learning\n",
    "# https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Plug our objective function into the standard optimizer and let it do it's thing\n",
    "#\n",
    "s = time.time()\n",
    "OptimizeResult = minimize(objective, np.zeros(num_features + 1), args=(data1, data_class1))\n",
    "beta_learned = OptimizeResult.x\n",
    "e = time.time()\n",
    "print('\\nDone learning... {}s'.format(e - s))\n",
    "\n",
    "print('BETAS...')\n",
    "print(beta_learned)\n",
    "\n",
    "print('accuracy on training data: {}'.format(accuracy(data1, data_class1, beta_learned)))\n",
    "\n",
    "print('test learned classifier')\n",
    "test_result(beta_learned, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare the parameters we learned from our own implementation vs the scipy minimizer\n",
    "# these values should be nearly identical since we're optimizing the same function in both cases\n",
    "print(betas)\n",
    "print(beta_learned)\n",
    "\n",
    "[-3.25695301  3.27061501 -0.53130681]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement logistic regression on the same data set using skitlearn and compare the results.\n",
    "\n",
    "# TODO: what happens if you increase the dimensionality of the input data?\n",
    "# Test it on a real datasets: https://archive.ics.uci.edu/ml/datasets/Adult\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
